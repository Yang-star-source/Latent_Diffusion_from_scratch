{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "id": "zacCV3p7QGcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define VAE"
      ],
      "metadata": {
        "id": "pMKt5yz-QYOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kleO_LF7PPZa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,dropout_prob = 0.0):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(MidBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,in_channels)\n",
        "        self.attn1 = AttentionBlock(in_channels)\n",
        "        self.res2 = ResNetBlock(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "        self.down_block = nn.Sequential(\n",
        "            DownBlock(64,128),\n",
        "            DownBlock(128,256),\n",
        "            DownBlock(256,512,has_attn = True)\n",
        "        )\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(512,out_channels*2,kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self,x):\n",
        "        mean , log_var = torch.chunk(x,2,dim=1)\n",
        "        log_var = torch.clamp(log_var, -30.0, 20.0)\n",
        "        D_kl = 0.5 *(torch.exp(log_var) + mean**2 - log_var - 1)\n",
        "        D_kl = torch.sum(D_kl,dim=[1,2,3]).mean() # Mean is for batch dimention\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mean + eps*std\n",
        "        return z,D_kl\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.down_block(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.out(x)\n",
        "        z,D_kl = self.reparameterize(x)\n",
        "        return z,D_kl\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), # nearest mode by default\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,512,kernel_size=3,padding=1)\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.up_block = nn.Sequential(\n",
        "            UpBlock(512,256,has_attn=True),\n",
        "            UpBlock(256,128),\n",
        "            UpBlock(128,64)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.up_block(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "class PatchGan(nn.Module):\n",
        "    def __init__(self,in_channels=3):\n",
        "        super(PatchGan, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Conv2d(in_channels,64,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,128),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,256),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(256,512,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(512,1,kernel_size=3,stride=1,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(in_channels,out_channels)\n",
        "        self.decoder = Decoder(out_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        z,D_kl = self.encoder(x)\n",
        "        x = self.decoder(z)\n",
        "        return x,D_kl\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training VAE"
      ],
      "metadata": {
        "id": "QyUxTEIuQciC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets , transforms\n",
        "from torchvision.utils import save_image\n",
        "import lpips\n",
        "import os\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# PERCEPTUAL LOSS used pretrained , for SHARPNESS\n",
        "lpips_loss_fn = lpips.LPIPS(net='vgg').to(DEVICE)\n",
        "\n",
        "IMAGES_PATH = \"/content/drive/MyDrive/dataset\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/VAE_Training\"\n",
        "LATENTS_PATH = \"/content/drive/MyDrive/VAE_Training/LATENTS\"\n",
        "os.makedirs(LATENTS_PATH,exist_ok=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(IMAGES_PATH,transform=transform)\n",
        "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "VAE_MODEL = VAE().to(DEVICE)\n",
        "PATCH_GAN = PatchGan().to(DEVICE)\n",
        "\n",
        "VAE_OPTIMIZER = torch.optim.Adam(VAE_MODEL.parameters(),lr=0.0002)\n",
        "PATCH_GAN_OPTIMIZER = torch.optim.Adam(PATCH_GAN.parameters(),lr=0.0001)\n",
        "\n",
        "L1_LOSS = nn.L1Loss()\n",
        "\n",
        "# Logits -> Sigmoid -> BCE Loss\n",
        "BCE_LOSS = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Weights from the Paper\n",
        "# High-Resolution Image Synthesis with Latent Diffusion Models\n",
        "# Page 29\n",
        "weight_kl = 0.000001\n",
        "weight_lpips = 1.0\n",
        "\n",
        "DISC_START = 10\n",
        "\n",
        "# ==============================================\n",
        "# LLM Generated HELPER FUNCTION: ADAPTIVE WEIGHT\n",
        "# ==============================================\n",
        "def calculate_adaptive_weight(recon_loss, g_loss, last_layer, weight_limit=1.0):\n",
        "    # gradient <- torch.autograd.grad (loss,weights)\n",
        "    # why [0] ? since obj returned is tuple , we need extract the tensor inside tuple\n",
        "    # why retain_graph = True ? It's map of how data flow\n",
        "    # keep for g_grads and VAE_LOSS.backward()\n",
        "    recon_grads = torch.autograd.grad(recon_loss, last_layer, retain_graph=True)[0]\n",
        "    g_grads = torch.autograd.grad(g_loss, last_layer, retain_graph=True)[0]\n",
        "\n",
        "    # apply L2 Norm to get the magnitude\n",
        "    d_weight = torch.norm(recon_grads) / (torch.norm(g_grads) + 1e-4)\n",
        "    d_weight = torch.clamp(d_weight, 0.0, 1.0).detach()\n",
        "    return d_weight\n",
        "\n",
        "def sampling_images(model,real_image,epoch,RECON_IMAGES_PATH = \"/content/drive/MyDrive/VAE_Training/RECON_IMAGES_SAMPLING\"):\n",
        "    os.makedirs(RECON_IMAGES_PATH,exist_ok=True)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        real_image = real_image.unsqueeze(0).to(DEVICE)\n",
        "        reconstructed_image , _ = model(real_image)\n",
        "\n",
        "        reconstructed_image = reconstructed_image*0.5 + 0.5\n",
        "        real_image = real_image*0.5 + 0.5\n",
        "\n",
        "        reconstructed_image = torch.clamp(reconstructed_image,0.0,1.0)\n",
        "        real_image = torch.clamp(real_image,0.0,1.0)\n",
        "\n",
        "        comparison_image = torch.cat([real_image,reconstructed_image],dim=3)\n",
        "        save_image(comparison_image,f\"{RECON_IMAGES_PATH}/comparison_image{epoch}.png\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for idx , (real_images,_) in enumerate(dataloader):\n",
        "        real_images = real_images.to(DEVICE)\n",
        "\n",
        "        '''\n",
        "        Train VAE\n",
        "        We want to let GAN predict on reconstructed images to be 1 ,\n",
        "        so GAN fooled , means loss on VAE lesser\n",
        "        '''\n",
        "\n",
        "        reconstructed_images , D_kl = VAE_MODEL(real_images)\n",
        "\n",
        "        l1_loss = L1_LOSS(reconstructed_images,real_images)\n",
        "        lpips_loss = lpips_loss_fn(reconstructed_images,real_images).mean()\n",
        "        rec_loss = l1_loss + (weight_lpips * lpips_loss)\n",
        "\n",
        "        # GAN predict 1 on 0 , means GAN fooled , VAE strong\n",
        "        gan_fake_images_pred = PATCH_GAN(reconstructed_images)\n",
        "        all_ones = torch.ones_like(gan_fake_images_pred)\n",
        "        gan_fake_images_pred_loss = BCE_LOSS(gan_fake_images_pred,all_ones)\n",
        "\n",
        "        # warmup , dont tell the VAE how GAN think on it's generated images first\n",
        "        # for 10 epochs\n",
        "        if epoch < DISC_START:\n",
        "            adaptive_weight = 0.0\n",
        "        else:\n",
        "            # Get the weight of the last layer of decoder\n",
        "            # self.decoder -> self.out -> last layer in nn.Sequential(...) -> get weight\n",
        "            last_layer = VAE_MODEL.decoder.out[-1].weight\n",
        "            adaptive_weight = calculate_adaptive_weight(rec_loss,\n",
        "                                                        gan_fake_images_pred_loss,\n",
        "                                                        last_layer)\n",
        "\n",
        "        VAE_LOSS = (weight_lpips * lpips_loss) + \\\n",
        "                   (weight_kl * D_kl) + \\\n",
        "                   (l1_loss) + \\\n",
        "                   (adaptive_weight* gan_fake_images_pred_loss)\n",
        "\n",
        "        VAE_OPTIMIZER.zero_grad()\n",
        "        VAE_LOSS.backward()\n",
        "        VAE_OPTIMIZER.step()\n",
        "\n",
        "        if epoch >= DISC_START:\n",
        "            gan_real_images_pred = PATCH_GAN(real_images)\n",
        "            all_ones = torch.ones_like(gan_real_images_pred)\n",
        "            gan_real_images_pred_loss = BCE_LOSS(gan_real_images_pred,all_ones)\n",
        "\n",
        "            gan_fake_images_pred = PATCH_GAN(reconstructed_images.detach())\n",
        "            all_zeros = torch.zeros_like(gan_fake_images_pred)\n",
        "            gan_fake_images_pred_loss = BCE_LOSS(gan_fake_images_pred,all_zeros)\n",
        "\n",
        "            total_gan_loss = (gan_real_images_pred_loss + gan_fake_images_pred_loss)\n",
        "\n",
        "            PATCH_GAN_OPTIMIZER.zero_grad()\n",
        "            total_gan_loss.backward()\n",
        "            PATCH_GAN_OPTIMIZER.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "\n",
        "        if epoch >= DISC_START:\n",
        "            disc_loss_val = total_gan_loss.item()\n",
        "        else:\n",
        "            disc_loss_val = 0.0\n",
        "\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"vae_state_dict\": VAE_MODEL.state_dict(),\n",
        "            \"gan_state_dict\": PATCH_GAN.state_dict(),\n",
        "            \"vae_optimizer\": VAE_OPTIMIZER.state_dict(),\n",
        "            \"gan_optimizer\": PATCH_GAN_OPTIMIZER.state_dict(),\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint,os.path.join(MODEL_PATH,\"checkpoint.pth\"))\n",
        "\n",
        "        print(f\"Epoch {epoch} | VAE Loss: {VAE_LOSS.item():.4f} | Disc Loss: {disc_loss_val:.4f}\")\n",
        "        sampling_images(VAE_MODEL,real_images[0],epoch)\n"
      ],
      "metadata": {
        "id": "tyP3_8QqQSwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Latents"
      ],
      "metadata": {
        "id": "PflZhOKgRAwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._prims_common import check\n",
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/VAE_Training/checkpoint.pth\"\n",
        "IMAGES_PATH = \"/content/drive/MyDrive/VAE_Training/cat_dataset\"\n",
        "LATENTS_PATH = \"/content/drive/MyDrive/VAE_Training/LATENTS\"\n",
        "os.makedirs(LATENTS_PATH,exist_ok=True)\n",
        "\n",
        "checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "VAE_MODEL = VAE().to(DEVICE)\n",
        "VAE_MODEL.load_state_dict(checkpoint[\"vae_state_dict\"])\n",
        "\n",
        "VAE_MODEL.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(IMAGES_PATH,transform=transform)\n",
        "dataloader = DataLoader(dataset,batch_size=1,shuffle=False)\n",
        "\n",
        "print(\"Generating Latents\")\n",
        "with torch.no_grad():\n",
        "    for idx , (real_images,_) in enumerate(dataloader):\n",
        "        real_images = real_images.to(DEVICE)\n",
        "        latent, _ = VAE_MODEL.encoder(real_images)\n",
        "\n",
        "        save_path = os.path.join(LATENTS_PATH,f\"latent_{idx}.pt\")\n",
        "\n",
        "        # save 3D tensor , remove batch dimention\n",
        "        torch.save(latent.squeeze(0).cpu(),save_path)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            print(f\"Processed {idx} images\")\n",
        "\n",
        "print(\"All images processed\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6IZOCuicQ_eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Diffusion Unet"
      ],
      "metadata": {
        "id": "xALstWxeRcQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_time_embedding(time , dim):\n",
        "    if not torch.is_tensor(time):\n",
        "        time = torch.tensor(time)\n",
        "\n",
        "    device=time.device\n",
        "    if time.ndim == 0:\n",
        "        time = time.unsqueeze(0).unsqueeze(1)\n",
        "    else: # This will be execute in training since t shape is (B)\n",
        "        time = time.unsqueeze(1)\n",
        "        # (B) -> (B,1)\n",
        "\n",
        "    # important to specify device\n",
        "    i=torch.arange(dim//2,device=device).float()\n",
        "    obj = (time)/(10000**(2*i/dim))\n",
        "    return torch.cat([torch.sin(obj),torch.cos(obj)],dim=1)\n",
        "\n",
        "class time_embedding(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim,dim)\n",
        "        )\n",
        "\n",
        "    def forward(self,X):\n",
        "        return self.net(X)\n",
        "\n",
        "class DiffusionResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,dropout_prob = 0.0 ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        # purpose of this projection is to match channel dim , before adding to x\n",
        "        self.time_proj = nn.Linear(time_emb_dim,out_channels)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # (B,C) -> (B,C,1,1)\n",
        "        # why silu ?\n",
        "        emb = self.time_proj(self.silu(time_emb))\n",
        "        x = x + emb[:, :, None, None]\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class DiffusionAttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class DiffusionDownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = DiffusionResNetBlock(in_channels,out_channels,time_emb_dim)\n",
        "        self.res2 = DiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = DiffusionAttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,time_emb):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "\n",
        "        skip_connection = x\n",
        "\n",
        "        x = self.down(x)\n",
        "        return x , skip_connection\n",
        "\n",
        "class DiffusionMidBlock(nn.Module):\n",
        "    def __init__(self,in_channels,time_emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res1 = DiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "        self.attn = DiffusionAttentionBlock(in_channels)\n",
        "        self.res2 = DiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "        x = self.res2(x,time_emb)\n",
        "        return x\n",
        "\n",
        "class DiffusionUpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        # Why input *2 ? Because we have to concatenate the channels\n",
        "        self.res1 = DiffusionResNetBlock(in_channels*2,out_channels,time_emb_dim)\n",
        "        self.res2 = DiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        if has_attn:\n",
        "            self.attn = DiffusionAttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,skip_connection,time_emb):\n",
        "        x = self.up(x)\n",
        "\n",
        "        # cancatenate at channels dimension\n",
        "        x = torch.cat([x,skip_connection],dim=1)\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "class DiffusionUnet(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=4,time_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_dim = time_dim\n",
        "        self.time_embedding = time_embedding(time_dim)\n",
        "\n",
        "        self.init_conv = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.down1 = DiffusionDownBlock(64,64,time_dim)\n",
        "        self.down2 = DiffusionDownBlock(64,128,time_dim)\n",
        "        self.down3 = DiffusionDownBlock(128,128,time_dim,has_attn=True)\n",
        "        self.down4 = DiffusionDownBlock(128,256,time_dim,has_attn=True)\n",
        "\n",
        "        self.mid = DiffusionMidBlock(256,time_dim)\n",
        "\n",
        "        self.up1 = DiffusionUpBlock(256,128,time_dim,has_attn=True)\n",
        "        self.up2 = DiffusionUpBlock(128,128,time_dim,has_attn=True)\n",
        "        self.up3 = DiffusionUpBlock(128,64,time_dim)\n",
        "        self.up4 = DiffusionUpBlock(64,64,time_dim)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        t = raw_time_embedding(t,self.time_dim)\n",
        "        emb = self.time_embedding(t)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        x1 , skip1 = self.down1(x,emb)\n",
        "        x2 , skip2 = self.down2(x1,emb)\n",
        "        x3 , skip3 = self.down3(x2,emb)\n",
        "        x4 , skip4 = self.down4(x3,emb)\n",
        "\n",
        "        x = self.mid(x4,emb)\n",
        "\n",
        "        x = self.up1(x,skip4,emb)\n",
        "        x = self.up2(x,skip3,emb)\n",
        "        x = self.up3(x,skip2,emb)\n",
        "        x = self.up4(x,skip1,emb)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cG7xKSVBRWfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Diffusion Unet"
      ],
      "metadata": {
        "id": "bHs1rsMmRmhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LATENTS_PATH = \"/content/drive/MyDrive/VAE_Training/LATENTS\"\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/VAE_Training/checkpoint.pth\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/VAE_Training/Diffusion_Model/VERSION4\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/VAE_Training/Diffusion_Model/VERSION4/ImageSampling\"\n",
        "# version 1 is Diffusion absolute from pretrained\n",
        "# version 2 is add dataset , train until unet_epoch_249\n",
        "# version 3 is use own defined add noice and denoice step\n",
        "# version 4 is use own defined diffusion unet and all add noice,denoice process\n",
        "os.makedirs(MODEL_SAVE_PATH,exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER,exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 500\n",
        "\n",
        "class LatentDataset(Dataset):\n",
        "    def __init__(self,root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.latent_files = [f for f in os.listdir(self.root_dir) if f.endswith('.pt')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.latent_files)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        latent_path = os.path.join(self.root_dir,self.latent_files[idx])\n",
        "        latent = torch.load(latent_path)\n",
        "        return latent\n",
        "\n",
        "dataset = LatentDataset(LATENTS_PATH)\n",
        "print(len(dataset))\n",
        "dataloader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "model = DiffusionUnet().to(DEVICE)\n",
        "vae = VAE().to(DEVICE)\n",
        "checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "vae.load_state_dict(checkpoint[\"vae_state_dict\"])\n",
        "vae.eval()\n",
        "\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, num_timesteps,device=DEVICE)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for step, latents in enumerate(progress_bar):\n",
        "        latents = latents.to(DEVICE)\n",
        "\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=DEVICE)\n",
        "\n",
        "        sqrt_alpha_cumprod = sqrt_alphas_cumprod[timesteps][:,None,None,None]\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[timesteps][:,None,None,None]\n",
        "\n",
        "        noisy_latents = sqrt_alpha_cumprod * latents + sqrt_one_minus_alpha_cumprod * noise\n",
        "\n",
        "        noise_pred = model(noisy_latents, timesteps)\n",
        "\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    # Save Checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {epoch_loss / len(dataloader)}\")\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        torch_path = os.path.join(MODEL_SAVE_PATH,f\"unet_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), torch_path)\n",
        "        print(f\"Saved model to epoch_{epoch}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            latents = torch.randn(\n",
        "                (8, 4, 32, 32),\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "            for t in reversed(range(1000)):\n",
        "                t_tensor = torch.ones(8,device=DEVICE).long() * t\n",
        "\n",
        "                noise_pred = model(latents, t_tensor)\n",
        "\n",
        "                alpha = alphas[t]\n",
        "                alpha_cumprod = alphas_cumprod[t]\n",
        "                beta = betas[t]\n",
        "                sqrt_alpha_cumprod = sqrt_alphas_cumprod[t]\n",
        "                sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
        "\n",
        "                if t >0:\n",
        "                    noise = torch.randn_like(latents)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(latents)\n",
        "\n",
        "                # Subtract noise\n",
        "                latents = (1 / torch.sqrt(alpha)) * (latents - ((1 - alpha) / (torch.sqrt(1 - alpha_cumprod))) * noise_pred) + torch.sqrt(beta) * noise\n",
        "\n",
        "            reconstructed = vae.decoder(latents)\n",
        "\n",
        "            # Un-normalize (-1,1 -> 0,1)\n",
        "            reconstructed = reconstructed * 0.5 + 0.5\n",
        "            reconstructed = torch.clamp(reconstructed, 0, 1)\n",
        "\n",
        "            #Save\n",
        "            save_image(reconstructed, f\"{OUTPUT_FOLDER}/generated_cats_v4{epoch+1}.png\", nrow=4)\n",
        "            print(f\"Saved to {OUTPUT_FOLDER}/generated_cats_v4{epoch+1}.png\")\n",
        "\n",
        "print(\"Training Complete!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lvm8-RjrRkIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling Generated Images"
      ],
      "metadata": {
        "id": "339lUNk2SVGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "VAE_CHECKPOINT = \"/content/drive/MyDrive/VAE_Training/checkpoint.pth\"\n",
        "UNET_PATH = \"/content/drive/MyDrive/VAE_Training/Diffusion_Model/VERSION4/unet_epoch_500.pth\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/VAE_Training/Diffusion_Model/VERSION4/ImageSampling\"\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "vae = VAE().to(DEVICE) # Ensure your VAE class is defined above!\n",
        "checkpoint = torch.load(VAE_CHECKPOINT, map_location=DEVICE)\n",
        "vae.load_state_dict(checkpoint[\"vae_state_dict\"])\n",
        "vae.eval()\n",
        "\n",
        "unet = DiffusionUnet().to(DEVICE)\n",
        "unet.load_state_dict(torch.load(UNET_PATH))\n",
        "unet.eval()\n",
        "\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, num_timesteps,device=DEVICE)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "with torch.no_grad():\n",
        "    latents = torch.randn(\n",
        "        (8, 4, 32, 32),\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    for t in reversed(range(1000)):\n",
        "        t_tensor = torch.ones(8,device=DEVICE).long() * t\n",
        "\n",
        "        noise_pred = unet(latents, t_tensor)\n",
        "\n",
        "        alpha = alphas[t]\n",
        "        alpha_cumprod = alphas_cumprod[t]\n",
        "        beta = betas[t]\n",
        "        sqrt_alpha_cumprod = sqrt_alphas_cumprod[t]\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
        "\n",
        "        if t >0:\n",
        "            noise = torch.randn_like(latents)\n",
        "        else:\n",
        "            noise = torch.zeros_like(latents)\n",
        "\n",
        "        # Subtract noise\n",
        "        latents = (1 / torch.sqrt(alpha)) * (latents - ((1 - alpha) / (torch.sqrt(1 - alpha_cumprod))) * noise_pred) + torch.sqrt(beta) * noise\n",
        "\n",
        "    reconstructed = vae.decoder(latents)\n",
        "\n",
        "    # Un-normalize (-1,1 -> 0,1)\n",
        "    reconstructed = reconstructed * 0.5 + 0.5\n",
        "    reconstructed = torch.clamp(reconstructed, 0, 1)\n",
        "\n",
        "    #Save image , nrow=4 means in 4 column , weird design\n",
        "    # save_image save number in integer [0,255] , but expect input [0.0,1.0]\n",
        "    save_image(reconstructed, f\"{OUTPUT_FOLDER}/sample_cat9.png\", nrow=4)\n",
        "    print(f\"Saved to {OUTPUT_FOLDER}/sample_cat9.png\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZDIgO_RoSM-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}